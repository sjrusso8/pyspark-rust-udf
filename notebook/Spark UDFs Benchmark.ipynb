{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a310bb-0603-448f-af5e-4b868694b9d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark UDF Compare\n",
    "\n",
    "This notebook is inspired by the original Databricks Blog post for [Introducing Pandas UDF for PySpark](https://www.databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html). For the original implementation of the benchmark, check the [Pandas UDF Notebook](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1281142885375883/2174302049319883/7729323681064935/latest.html).\n",
    "\n",
    "## Background\n",
    "\n",
    "I wanted to see if I could rewrite the same functions used in the original blog post in Rust and do the same speed comparison. The point of this is not to prove that you should rewrite your Spark UDFs in Rust, but rather to show it is possible (but probably not the best idea). I kept my rust code as vanilla as possible. This is not to compare other rust created frameworks like Polars or Ballista. \n",
    "\n",
    "## Changes\n",
    "\n",
    "I did *slightly* modify the original notebook. I adjusted the formatting (imports, multiline statements, etc.), and updated the since to match the new Pandas UDF interface for Spark 3.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678d87fc-33ff-4725-a009-3a20aa6b565d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f2a528-0dc9-45ab-a144-f9a6ba108af8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To get the pip install to work correctly\n",
    "# I had to install cargo and the pip package directly on the driver using the web terminal \n",
    "# Using the web terminal, run the following command\n",
    "# curl https://sh.rustup.rs -sSf | sh -s -- -y && source \"$HOME/.cargo/env\" && pip install rust-udf-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e2009c-f4da-4805-9bcb-86ea68977f62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from rust_udf import plus_one_rs, cdf_rs, subtract_mean_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37504407-676f-4d3c-a214-9dfc2b8cda93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 10000000\n+---+-------------------+\n| id|                  v|\n+---+-------------------+\n|  0|0.15651868958313198|\n|  0|0.21716444334624274|\n|  0| 0.8683173173207329|\n|  0|0.07800200127201706|\n|  0| 0.6932806235978584|\n|  0| 0.8377408833427843|\n|  0| 0.8538987778780405|\n|  0|0.25537961838485645|\n|  0| 0.2631392753730042|\n|  0|0.42540886550134116|\n|  0|0.43707222732694373|\n|  0| 0.7095514823050232|\n|  0|   0.65171189281833|\n|  0|0.16387925660322944|\n|  0|0.42694974747514103|\n|  0|0.13721004432109374|\n|  0| 0.1174495995075876|\n|  0|0.05475323785989383|\n|  0| 0.3431083493195717|\n|  0| 0.3549490254636438|\n+---+-------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.range(0, 10 * 1000 * 1000)\n",
    "    .withColumn(\"id\", (F.col(\"id\") / 10000).cast(\"integer\"))\n",
    "    .withColumn(\"v\", F.rand())\n",
    ")\n",
    "\n",
    "df.cache()\n",
    "\n",
    "print(\"Number of records:\", df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360c8f0e-05ab-4a37-bda0-f159e61b92f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n3.48 s ± 141 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@udf('double')\n",
    "def plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "%timeit df.withColumn('v', plus_one(df.v)).agg(F.count(F.col('v'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89eec74c-f760-4523-8c3c-1348273c39d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n1.77 s ± 409 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def pandas_plus_one(v: pd.Series) -> pd.Series:\n",
    "    return v + 1\n",
    "\n",
    "%timeit df.withColumn('v', pandas_plus_one(df.v)).agg(F.count(F.col('v'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac1098d-1c0d-46bb-8cc9-e7afa1c37cac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n3.9 s ± 61.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Rust plus_one\n",
    "@udf('double')\n",
    "def plus_one(v):\n",
    "    return plus_one_rs(v)\n",
    "\n",
    "%timeit df.withColumn('v', plus_one(df.v)).agg(F.count(F.col('v'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393e4e00-de0e-4957-aee8-2086ddf4af17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n3min 8s ± 890 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@udf('double')\n",
    "def cdf(v):\n",
    "    return float(stats.norm.cdf(v))\n",
    "\n",
    "%timeit df.withColumn('cumulative_probability', cdf(df.v)).agg(F.count(F.col('cumulative_probability'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7d6c32-b9cc-4544-84f4-0bb52133efad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n1.55 s ± 72 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('double')\n",
    "def pandas_cdf(v: pd.Series) -> pd.Series:\n",
    "    return pd.Series(stats.norm.cdf(v))\n",
    "\n",
    "%timeit df.withColumn('cumulative_probability', pandas_cdf(df.v)).agg(F.count(F.col('cumulative_probability'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385007d7-8d16-4ec9-a396-ed470c72526a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n+-----------------------------+\n|count(cumulative_probability)|\n+-----------------------------+\n|                     10000000|\n+-----------------------------+\n\n4.11 s ± 112 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Rust cdf\n",
    "@udf('double')\n",
    "def cdf(v):\n",
    "  return cdf_rs(v)\n",
    "\n",
    "%timeit df.withColumn('cumulative_probability', cdf(df.v)).agg(F.count(F.col('cumulative_probability'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e84d0c78-ecd9-4d91-a14c-73121cdec410",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n40.9 s ± 1.78 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@udf(T.ArrayType(df.schema))\n",
    "def substract_mean(rows):\n",
    "    vs = pd.Series([r.v for r in rows])\n",
    "    vs = vs - vs.mean()\n",
    "    return [Row(id=rows[i]['id'], v=float(vs[i])) for i in range(len(rows))]\n",
    "\n",
    "%timeit df.groupby('id')\\\n",
    "  .agg(F.collect_list(F.struct(df['id'], df['v'])).alias('rows'))\\\n",
    "  .withColumn('new_rows', substract_mean(F.col('rows')))\\\n",
    "  .withColumn('new_row', F.explode(F.col('new_rows')))\\\n",
    "  .withColumn('id', F.col('new_row.id'))\\\n",
    "  .withColumn('v', F.col('new_row.v'))\\\n",
    "  .agg(F.count(F.col('v'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "770300bc-009b-42dc-92c6-d008b1b5fa3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n+--------+\n|count(v)|\n+--------+\n|10000000|\n+--------+\n\n2.82 s ± 124 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def pandas_subtract_mean(pdf):\n",
    "\treturn pdf.assign(v=pdf.v - pdf.v.mean())\n",
    "\n",
    "%timeit df.groupby('id').applyInPandas(pandas_subtract_mean, schema=df.schema).agg(F.count(F.col('v'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c763b434-fa76-4695-8d5f-6b129a1f8b3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n+----------------+\n|count(v_explode)|\n+----------------+\n|        10000000|\n+----------------+\n\n3.44 s ± 97.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Rust subtract mean\n",
    "@udf(T.ArrayType(T.FloatType()))\n",
    "def subtract_mean(v):\n",
    "\treturn subtract_mean_rs(v)\n",
    "\n",
    "%timeit df.groupby('id')\\\n",
    "  .agg(F.collect_list('v').alias('v_list'))\\\n",
    "  .withColumn('v_new', subtract_mean(F.col('v_list')))\\\n",
    "  .withColumn('v_explode', F.explode(F.col('v_new')))\\\n",
    "  .agg(F.count(F.col('v_explode'))).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2689309537493581,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark UDFs Benchmark",
   "notebookOrigID": 1603387415991055,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
